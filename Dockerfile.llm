FROM python:3.10-slim

WORKDIR /app

# Install system dependencies including those needed for llama-cpp-python
RUN apt-get update && \
    apt-get install -y --no-install-recommends \
    build-essential \
    cmake \
    curl \
    git \
    python3-dev \
    && apt-get clean \
    && rm -rf /var/lib/apt/lists/*

# Set environment variables for llama-cpp-python build
ENV CMAKE_ARGS="-DLLAMA_CUBLAS=OFF" 
ENV FORCE_CMAKE=1
ENV PIP_DISABLE_PIP_VERSION_CHECK=1 
ENV PIP_NO_CACHE_DIR=1

# Copy requirements file
COPY requirements.txt .

# Install base requirements
RUN pip install -r requirements.txt

# Install llama-index and related packages with specific compatible versions
RUN pip install llama-index-core==0.10.11 
RUN pip install llama-index-llms-llama-cpp==0.1.1
RUN pip install llama-index-embeddings-huggingface==0.1.5
RUN pip install llama-index-vector-stores-chroma==0.1.1

# Create necessary directories
RUN mkdir -p models vector_store logs

# Download model (cached in models directory)
RUN mkdir -p /app/models && \
    cd /app/models && \
    MODEL_URL=${CURRENT_LLM:-"https://huggingface.co/bartowski/Qwen2.5-1.5B-Instruct-GGUF/resolve/main/Qwen2.5-1.5B-Instruct-Q4_K_M.gguf"} && \
    MODEL_FILENAME=$(basename $MODEL_URL) && \
    if [ ! -f "$MODEL_FILENAME" ]; then \
        echo "Downloading model $MODEL_URL"; \
        curl -L -o "$MODEL_FILENAME" "$MODEL_URL"; \
    else \
        echo "Model already exists: $MODEL_FILENAME"; \
    fi

# Copy application code
COPY . .

# Add health check
HEALTHCHECK --interval=30s --timeout=10s --start-period=40s --retries=3 \
    CMD curl -f http://localhost:5000/health || exit 1

# Entry point
CMD ["python", "src/llm_service.py"]
